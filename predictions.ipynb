{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T01:38:53.884741Z",
     "start_time": "2025-11-16T01:38:53.858290Z"
    },
    "execution": {
     "iopub.execute_input": "2024-02-12T09:09:38.155607Z",
     "iopub.status.busy": "2024-02-12T09:09:38.155145Z",
     "iopub.status.idle": "2024-02-12T09:09:52.386031Z",
     "shell.execute_reply": "2024-02-12T09:09:52.385085Z",
     "shell.execute_reply.started": "2024-02-12T09:09:38.155571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.3.228)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.9.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (0.24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (7.1.3)\n",
      "Requirement already satisfied: polars in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.35.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from polars->ultralytics) (1.35.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dill in c:\\users\\paulo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the 'ultralytics' package using pip\n",
    "%pip install ultralytics\n",
    "\n",
    "# Install dill package (required for loading the model)\n",
    "%pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T09:09:52.388650Z",
     "iopub.status.busy": "2024-02-12T09:09:52.388258Z",
     "iopub.status.idle": "2024-02-12T09:09:59.490521Z",
     "shell.execute_reply": "2024-02-12T09:09:59.489745Z",
     "shell.execute_reply.started": "2024-02-12T09:09:52.388617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import the YOLO module from the ultralytics library.\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T09:09:59.491858Z",
     "iopub.status.busy": "2024-02-12T09:09:59.491453Z",
     "iopub.status.idle": "2024-02-12T09:09:59.677860Z",
     "shell.execute_reply": "2024-02-12T09:09:59.677049Z",
     "shell.execute_reply.started": "2024-02-12T09:09:59.491833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of the YOLO model\n",
    "# Initialize it with the pre-trained weights file 'best.pt' located at the specified path\n",
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T09:09:59.680930Z",
     "iopub.status.busy": "2024-02-12T09:09:59.680172Z",
     "iopub.status.idle": "2024-02-12T09:09:59.684809Z",
     "shell.execute_reply": "2024-02-12T09:09:59.683915Z",
     "shell.execute_reply.started": "2024-02-12T09:09:59.680898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define path to video file\n",
    "source = 'input/A001_09031158_C021.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T09:09:59.686573Z",
     "iopub.status.busy": "2024-02-12T09:09:59.686274Z",
     "iopub.status.idle": "2024-02-12T09:10:00.380404Z",
     "shell.execute_reply": "2024-02-12T09:10:00.379554Z",
     "shell.execute_reply.started": "2024-02-12T09:09:59.686540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run inference on the source and save results with H.265 codec\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Note: OpenCV needs to be configured with H.265 support\u001b[39;00m\n\u001b[32m      3\u001b[39m results = model.predict(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     source=\u001b[43msource\u001b[49m, \n\u001b[32m      5\u001b[39m     save=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m      6\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# The video will be saved as .mp4 by default\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# For H.265 encoding, you may need to process the output separately with ffmpeg\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'source' is not defined"
     ]
    }
   ],
   "source": [
    "# Run inference on the source and save results with H.265 codec\n",
    "# Note: OpenCV needs to be configured with H.265 support\n",
    "results = model.predict(\n",
    "    source=source, \n",
    "    save=True, \n",
    "    stream=True,\n",
    "    # The video will be saved as .mp4 by default\n",
    "    # For H.265 encoding, you may need to process the output separately with ffmpeg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T09:10:00.381793Z",
     "iopub.status.busy": "2024-02-12T09:10:00.381494Z",
     "iopub.status.idle": "2024-02-12T09:10:23.145380Z",
     "shell.execute_reply": "2024-02-12T09:10:23.144667Z",
     "shell.execute_reply.started": "2024-02-12T09:10:00.381768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (frame 1/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 16 Humans, 60.1ms\n",
      "video 1/1 (frame 2/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 113.5ms\n",
      "video 1/1 (frame 3/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 45.0ms\n",
      "video 1/1 (frame 4/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 44.3ms\n",
      "video 1/1 (frame 5/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 42.6ms\n",
      "video 1/1 (frame 6/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 59.4ms\n",
      "video 1/1 (frame 7/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 47.4ms\n",
      "video 1/1 (frame 8/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 46.4ms\n",
      "video 1/1 (frame 9/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 41.2ms\n",
      "video 1/1 (frame 10/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 38.9ms\n",
      "video 1/1 (frame 11/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 46.1ms\n",
      "video 1/1 (frame 12/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 50.7ms\n",
      "video 1/1 (frame 13/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 47.5ms\n",
      "video 1/1 (frame 14/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 47.4ms\n",
      "video 1/1 (frame 15/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 43.7ms\n",
      "video 1/1 (frame 16/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 41.2ms\n",
      "video 1/1 (frame 17/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 41.0ms\n",
      "video 1/1 (frame 18/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 44.5ms\n",
      "video 1/1 (frame 19/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 39.7ms\n",
      "video 1/1 (frame 20/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 43.4ms\n",
      "video 1/1 (frame 21/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 16 Humans, 49.8ms\n",
      "video 1/1 (frame 22/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 18 Humans, 46.6ms\n",
      "video 1/1 (frame 23/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 47.0ms\n",
      "video 1/1 (frame 24/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 17 Humans, 42.9ms\n",
      "video 1/1 (frame 25/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 19 Humans, 43.4ms\n",
      "video 1/1 (frame 26/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 49.1ms\n",
      "video 1/1 (frame 27/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 44.6ms\n",
      "video 1/1 (frame 28/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 47.4ms\n",
      "video 1/1 (frame 29/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 49.4ms\n",
      "video 1/1 (frame 30/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 38.0ms\n",
      "video 1/1 (frame 31/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 38.7ms\n",
      "video 1/1 (frame 32/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.9ms\n",
      "video 1/1 (frame 33/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 44.1ms\n",
      "video 1/1 (frame 34/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 43.1ms\n",
      "video 1/1 (frame 35/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 41.9ms\n",
      "video 1/1 (frame 36/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 43.9ms\n",
      "video 1/1 (frame 37/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 46.5ms\n",
      "video 1/1 (frame 38/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.9ms\n",
      "video 1/1 (frame 39/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 40.6ms\n",
      "video 1/1 (frame 40/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 37.9ms\n",
      "video 1/1 (frame 41/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 40.4ms\n",
      "video 1/1 (frame 42/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 37.3ms\n",
      "video 1/1 (frame 43/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 36.9ms\n",
      "video 1/1 (frame 44/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 42.6ms\n",
      "video 1/1 (frame 45/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 43.4ms\n",
      "video 1/1 (frame 46/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 40.7ms\n",
      "video 1/1 (frame 47/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.0ms\n",
      "video 1/1 (frame 48/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.8ms\n",
      "video 1/1 (frame 49/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.0ms\n",
      "video 1/1 (frame 50/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 50.7ms\n",
      "video 1/1 (frame 51/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 46.7ms\n",
      "video 1/1 (frame 52/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 43.2ms\n",
      "video 1/1 (frame 53/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 39.3ms\n",
      "video 1/1 (frame 54/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 37.8ms\n",
      "video 1/1 (frame 55/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 36.3ms\n",
      "video 1/1 (frame 56/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 37.5ms\n",
      "video 1/1 (frame 57/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 41.2ms\n",
      "video 1/1 (frame 58/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 40.2ms\n",
      "video 1/1 (frame 59/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 44.8ms\n",
      "video 1/1 (frame 60/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 51.8ms\n",
      "video 1/1 (frame 61/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 48.3ms\n",
      "video 1/1 (frame 62/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.9ms\n",
      "video 1/1 (frame 63/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 45.2ms\n",
      "video 1/1 (frame 64/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 39.5ms\n",
      "video 1/1 (frame 65/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 41.6ms\n",
      "video 1/1 (frame 66/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 48.7ms\n",
      "video 1/1 (frame 67/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 53.8ms\n",
      "video 1/1 (frame 68/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 36.4ms\n",
      "video 1/1 (frame 69/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 56.9ms\n",
      "video 1/1 (frame 70/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 47.1ms\n",
      "video 1/1 (frame 71/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 80.5ms\n",
      "video 1/1 (frame 72/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 57.4ms\n",
      "video 1/1 (frame 73/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 38.4ms\n",
      "video 1/1 (frame 74/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 44.2ms\n",
      "video 1/1 (frame 75/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 44.7ms\n",
      "video 1/1 (frame 76/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 50.7ms\n",
      "video 1/1 (frame 77/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 41.3ms\n",
      "video 1/1 (frame 78/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 46.4ms\n",
      "video 1/1 (frame 79/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 40.1ms\n",
      "video 1/1 (frame 80/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 39.9ms\n",
      "video 1/1 (frame 81/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 42.5ms\n",
      "video 1/1 (frame 82/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 16 Humans, 50.4ms\n",
      "video 1/1 (frame 83/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 49.4ms\n",
      "video 1/1 (frame 84/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 44.1ms\n",
      "video 1/1 (frame 85/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 309.8ms\n",
      "video 1/1 (frame 86/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 16 Humans, 182.6ms\n",
      "video 1/1 (frame 87/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 152.9ms\n",
      "video 1/1 (frame 88/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 166.1ms\n",
      "video 1/1 (frame 89/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 14 Humans, 163.0ms\n",
      "video 1/1 (frame 90/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 155.4ms\n",
      "video 1/1 (frame 91/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 152.8ms\n",
      "video 1/1 (frame 92/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 152.7ms\n",
      "video 1/1 (frame 93/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 55.6ms\n",
      "video 1/1 (frame 94/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 65.2ms\n",
      "video 1/1 (frame 95/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 69.8ms\n",
      "video 1/1 (frame 96/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 45.9ms\n",
      "video 1/1 (frame 97/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 44.4ms\n",
      "video 1/1 (frame 98/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 42.3ms\n",
      "video 1/1 (frame 99/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 64.1ms\n",
      "video 1/1 (frame 100/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 41.1ms\n",
      "video 1/1 (frame 101/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 46.5ms\n",
      "video 1/1 (frame 102/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 47.1ms\n",
      "video 1/1 (frame 103/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.8ms\n",
      "video 1/1 (frame 104/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 44.8ms\n",
      "video 1/1 (frame 105/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 49.9ms\n",
      "video 1/1 (frame 106/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 37.7ms\n",
      "video 1/1 (frame 107/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 45.3ms\n",
      "video 1/1 (frame 108/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 52.1ms\n",
      "video 1/1 (frame 109/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 13 Humans, 61.4ms\n",
      "video 1/1 (frame 110/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 58.7ms\n",
      "video 1/1 (frame 111/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 65.3ms\n",
      "video 1/1 (frame 112/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 40.5ms\n",
      "video 1/1 (frame 113/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 40.0ms\n",
      "video 1/1 (frame 114/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 40.0ms\n",
      "video 1/1 (frame 115/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 42.8ms\n",
      "video 1/1 (frame 116/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 39.3ms\n",
      "video 1/1 (frame 117/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 36.3ms\n",
      "video 1/1 (frame 118/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 41.0ms\n",
      "video 1/1 (frame 119/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 36.7ms\n",
      "video 1/1 (frame 120/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 46.7ms\n",
      "video 1/1 (frame 121/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 42.9ms\n",
      "video 1/1 (frame 122/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 44.9ms\n",
      "video 1/1 (frame 123/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 39.5ms\n",
      "video 1/1 (frame 124/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 45.6ms\n",
      "video 1/1 (frame 125/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 39.6ms\n",
      "video 1/1 (frame 126/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 34.5ms\n",
      "video 1/1 (frame 127/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 37.4ms\n",
      "video 1/1 (frame 128/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 37.1ms\n",
      "video 1/1 (frame 129/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 45.8ms\n",
      "video 1/1 (frame 130/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 37.9ms\n",
      "video 1/1 (frame 131/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 44.0ms\n",
      "video 1/1 (frame 132/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 44.6ms\n",
      "video 1/1 (frame 133/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 41.8ms\n",
      "video 1/1 (frame 134/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 38.6ms\n",
      "video 1/1 (frame 135/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 37.8ms\n",
      "video 1/1 (frame 136/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.6ms\n",
      "video 1/1 (frame 137/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 39.2ms\n",
      "video 1/1 (frame 138/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 38.5ms\n",
      "video 1/1 (frame 139/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 37.9ms\n",
      "video 1/1 (frame 140/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 38.2ms\n",
      "video 1/1 (frame 141/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 50.4ms\n",
      "video 1/1 (frame 142/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 38.7ms\n",
      "video 1/1 (frame 143/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 43.4ms\n",
      "video 1/1 (frame 144/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 49.3ms\n",
      "video 1/1 (frame 145/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.1ms\n",
      "video 1/1 (frame 146/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 38.9ms\n",
      "video 1/1 (frame 147/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 38.3ms\n",
      "video 1/1 (frame 148/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 43.2ms\n",
      "video 1/1 (frame 149/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 37.1ms\n",
      "video 1/1 (frame 150/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 44.4ms\n",
      "video 1/1 (frame 151/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 39.3ms\n",
      "video 1/1 (frame 152/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.7ms\n",
      "video 1/1 (frame 153/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 37.8ms\n",
      "video 1/1 (frame 154/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 45.5ms\n",
      "video 1/1 (frame 155/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.7ms\n",
      "video 1/1 (frame 156/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 53.8ms\n",
      "video 1/1 (frame 157/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 39.9ms\n",
      "video 1/1 (frame 158/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 57.5ms\n",
      "video 1/1 (frame 159/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 60.8ms\n",
      "video 1/1 (frame 160/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 53.6ms\n",
      "video 1/1 (frame 161/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 58.8ms\n",
      "video 1/1 (frame 162/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 49.0ms\n",
      "video 1/1 (frame 163/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 37.0ms\n",
      "video 1/1 (frame 164/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.0ms\n",
      "video 1/1 (frame 165/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 41.4ms\n",
      "video 1/1 (frame 166/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 42.6ms\n",
      "video 1/1 (frame 167/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 40.4ms\n",
      "video 1/1 (frame 168/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 45.5ms\n",
      "video 1/1 (frame 169/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 39.9ms\n",
      "video 1/1 (frame 170/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 51.8ms\n",
      "video 1/1 (frame 171/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 35.7ms\n",
      "video 1/1 (frame 172/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 36.1ms\n",
      "video 1/1 (frame 173/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.1ms\n",
      "video 1/1 (frame 174/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 35.8ms\n",
      "video 1/1 (frame 175/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 41.9ms\n",
      "video 1/1 (frame 176/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 43.8ms\n",
      "video 1/1 (frame 177/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 41.3ms\n",
      "video 1/1 (frame 178/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 42.7ms\n",
      "video 1/1 (frame 179/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 36.9ms\n",
      "video 1/1 (frame 180/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 38.2ms\n",
      "video 1/1 (frame 181/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 46.4ms\n",
      "video 1/1 (frame 182/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 53.0ms\n",
      "video 1/1 (frame 183/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 34.7ms\n",
      "video 1/1 (frame 184/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 36.4ms\n",
      "video 1/1 (frame 185/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 43.6ms\n",
      "video 1/1 (frame 186/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 39.6ms\n",
      "video 1/1 (frame 187/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 45.6ms\n",
      "video 1/1 (frame 188/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 41.5ms\n",
      "video 1/1 (frame 189/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 42.8ms\n",
      "video 1/1 (frame 190/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 15 Humans, 41.2ms\n",
      "video 1/1 (frame 191/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 46.2ms\n",
      "video 1/1 (frame 192/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 40.4ms\n",
      "video 1/1 (frame 193/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 43.4ms\n",
      "video 1/1 (frame 194/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 40.5ms\n",
      "video 1/1 (frame 195/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 43.0ms\n",
      "video 1/1 (frame 196/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 41.1ms\n",
      "video 1/1 (frame 197/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 39.1ms\n",
      "video 1/1 (frame 198/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 37.3ms\n",
      "video 1/1 (frame 199/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 47.9ms\n",
      "video 1/1 (frame 200/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.9ms\n",
      "video 1/1 (frame 201/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 46.9ms\n",
      "video 1/1 (frame 202/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 36.4ms\n",
      "video 1/1 (frame 203/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 49.6ms\n",
      "video 1/1 (frame 204/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.2ms\n",
      "video 1/1 (frame 205/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 44.2ms\n",
      "video 1/1 (frame 206/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 40.0ms\n",
      "video 1/1 (frame 207/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 45.5ms\n",
      "video 1/1 (frame 208/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.7ms\n",
      "video 1/1 (frame 209/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.4ms\n",
      "video 1/1 (frame 210/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 40.7ms\n",
      "video 1/1 (frame 211/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 40.8ms\n",
      "video 1/1 (frame 212/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 39.1ms\n",
      "video 1/1 (frame 213/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 52.0ms\n",
      "video 1/1 (frame 214/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 41.8ms\n",
      "video 1/1 (frame 215/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.5ms\n",
      "video 1/1 (frame 216/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.7ms\n",
      "video 1/1 (frame 217/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 38.5ms\n",
      "video 1/1 (frame 218/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 46.3ms\n",
      "video 1/1 (frame 219/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 43.0ms\n",
      "video 1/1 (frame 220/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 58.9ms\n",
      "video 1/1 (frame 221/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 37.1ms\n",
      "video 1/1 (frame 222/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.4ms\n",
      "video 1/1 (frame 223/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 35.8ms\n",
      "video 1/1 (frame 224/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 38.0ms\n",
      "video 1/1 (frame 225/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 38.4ms\n",
      "video 1/1 (frame 226/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 41.4ms\n",
      "video 1/1 (frame 227/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 42.5ms\n",
      "video 1/1 (frame 228/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 37.0ms\n",
      "video 1/1 (frame 229/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 46.9ms\n",
      "video 1/1 (frame 230/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 41.6ms\n",
      "video 1/1 (frame 231/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 52.8ms\n",
      "video 1/1 (frame 232/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 38.8ms\n",
      "video 1/1 (frame 233/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 53.9ms\n",
      "video 1/1 (frame 234/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 39.0ms\n",
      "video 1/1 (frame 235/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 52.4ms\n",
      "video 1/1 (frame 236/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 45.9ms\n",
      "video 1/1 (frame 237/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.6ms\n",
      "video 1/1 (frame 238/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 41.2ms\n",
      "video 1/1 (frame 239/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 37.0ms\n",
      "video 1/1 (frame 240/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 40.0ms\n",
      "video 1/1 (frame 241/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 65.5ms\n",
      "video 1/1 (frame 242/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 63.4ms\n",
      "video 1/1 (frame 243/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.4ms\n",
      "video 1/1 (frame 244/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 48.8ms\n",
      "video 1/1 (frame 245/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 52.5ms\n",
      "video 1/1 (frame 246/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 49.4ms\n",
      "video 1/1 (frame 247/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 47.4ms\n",
      "video 1/1 (frame 248/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.1ms\n",
      "video 1/1 (frame 249/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 52.0ms\n",
      "video 1/1 (frame 250/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 43.2ms\n",
      "video 1/1 (frame 251/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 63.2ms\n",
      "video 1/1 (frame 252/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 56.4ms\n",
      "video 1/1 (frame 253/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 56.2ms\n",
      "video 1/1 (frame 254/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 44.4ms\n",
      "video 1/1 (frame 255/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 67.9ms\n",
      "video 1/1 (frame 256/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 46.8ms\n",
      "video 1/1 (frame 257/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 63.3ms\n",
      "video 1/1 (frame 258/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 61.4ms\n",
      "video 1/1 (frame 259/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 48.5ms\n",
      "video 1/1 (frame 260/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 41.6ms\n",
      "video 1/1 (frame 261/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 12 Humans, 63.9ms\n",
      "video 1/1 (frame 262/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 46.5ms\n",
      "video 1/1 (frame 263/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 63.2ms\n",
      "video 1/1 (frame 264/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 48.9ms\n",
      "video 1/1 (frame 265/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 58.3ms\n",
      "video 1/1 (frame 266/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 69.3ms\n",
      "video 1/1 (frame 267/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 42.4ms\n",
      "video 1/1 (frame 268/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 45.8ms\n",
      "video 1/1 (frame 269/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 40.6ms\n",
      "video 1/1 (frame 270/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 49.2ms\n",
      "video 1/1 (frame 271/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 43.2ms\n",
      "video 1/1 (frame 272/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 44.6ms\n",
      "video 1/1 (frame 273/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 56.3ms\n",
      "video 1/1 (frame 274/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 46.3ms\n",
      "video 1/1 (frame 275/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 47.6ms\n",
      "video 1/1 (frame 276/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 57.1ms\n",
      "video 1/1 (frame 277/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 46.5ms\n",
      "video 1/1 (frame 278/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 51.0ms\n",
      "video 1/1 (frame 279/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 60.1ms\n",
      "video 1/1 (frame 280/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 66.3ms\n",
      "video 1/1 (frame 281/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 62.6ms\n",
      "video 1/1 (frame 282/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 50.6ms\n",
      "video 1/1 (frame 283/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 40.6ms\n",
      "video 1/1 (frame 284/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 43.9ms\n",
      "video 1/1 (frame 285/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 62.1ms\n",
      "video 1/1 (frame 286/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 58.8ms\n",
      "video 1/1 (frame 287/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 48.2ms\n",
      "video 1/1 (frame 288/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 65.3ms\n",
      "video 1/1 (frame 289/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 64.5ms\n",
      "video 1/1 (frame 290/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 58.3ms\n",
      "video 1/1 (frame 291/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 44.0ms\n",
      "video 1/1 (frame 292/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 47.5ms\n",
      "video 1/1 (frame 293/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 45.6ms\n",
      "video 1/1 (frame 294/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 67.3ms\n",
      "video 1/1 (frame 295/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 61.8ms\n",
      "video 1/1 (frame 296/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 51.2ms\n",
      "video 1/1 (frame 297/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 64.8ms\n",
      "video 1/1 (frame 298/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 49.2ms\n",
      "video 1/1 (frame 299/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 42.1ms\n",
      "video 1/1 (frame 300/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 54.9ms\n",
      "video 1/1 (frame 301/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.7ms\n",
      "video 1/1 (frame 302/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 48.0ms\n",
      "video 1/1 (frame 303/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 55.6ms\n",
      "video 1/1 (frame 304/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 61.0ms\n",
      "video 1/1 (frame 305/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 44.4ms\n",
      "video 1/1 (frame 306/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 48.6ms\n",
      "video 1/1 (frame 307/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 41.9ms\n",
      "video 1/1 (frame 308/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 53.4ms\n",
      "video 1/1 (frame 309/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 64.8ms\n",
      "video 1/1 (frame 310/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 46.8ms\n",
      "video 1/1 (frame 311/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 56.1ms\n",
      "video 1/1 (frame 312/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 44.0ms\n",
      "video 1/1 (frame 313/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 48.5ms\n",
      "video 1/1 (frame 314/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 66.1ms\n",
      "video 1/1 (frame 315/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 41.0ms\n",
      "video 1/1 (frame 316/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.4ms\n",
      "video 1/1 (frame 317/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 50.3ms\n",
      "video 1/1 (frame 318/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 49.4ms\n",
      "video 1/1 (frame 319/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 45.0ms\n",
      "video 1/1 (frame 320/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 51.4ms\n",
      "video 1/1 (frame 321/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 44.4ms\n",
      "video 1/1 (frame 322/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 56.8ms\n",
      "video 1/1 (frame 323/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 40.3ms\n",
      "video 1/1 (frame 324/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 64.3ms\n",
      "video 1/1 (frame 325/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 50.2ms\n",
      "video 1/1 (frame 326/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 61.2ms\n",
      "video 1/1 (frame 327/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 63.6ms\n",
      "video 1/1 (frame 328/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 46.4ms\n",
      "video 1/1 (frame 329/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 58.2ms\n",
      "video 1/1 (frame 330/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 76.0ms\n",
      "video 1/1 (frame 331/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 59.0ms\n",
      "video 1/1 (frame 332/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 67.4ms\n",
      "video 1/1 (frame 333/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 45.4ms\n",
      "video 1/1 (frame 334/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 48.1ms\n",
      "video 1/1 (frame 335/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 63.8ms\n",
      "video 1/1 (frame 336/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 49.4ms\n",
      "video 1/1 (frame 337/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 50.5ms\n",
      "video 1/1 (frame 338/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 52.8ms\n",
      "video 1/1 (frame 339/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 40.4ms\n",
      "video 1/1 (frame 340/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 48.3ms\n",
      "video 1/1 (frame 341/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 49.6ms\n",
      "video 1/1 (frame 342/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 40.6ms\n",
      "video 1/1 (frame 343/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 42.7ms\n",
      "video 1/1 (frame 344/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 72.0ms\n",
      "video 1/1 (frame 345/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 41.5ms\n",
      "video 1/1 (frame 346/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 43.0ms\n",
      "video 1/1 (frame 347/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 67.0ms\n",
      "video 1/1 (frame 348/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 47.5ms\n",
      "video 1/1 (frame 349/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 53.1ms\n",
      "video 1/1 (frame 350/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 51.0ms\n",
      "video 1/1 (frame 351/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 44.2ms\n",
      "video 1/1 (frame 352/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 49.8ms\n",
      "video 1/1 (frame 353/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 48.5ms\n",
      "video 1/1 (frame 354/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 45.9ms\n",
      "video 1/1 (frame 355/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 45.1ms\n",
      "video 1/1 (frame 356/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 46.7ms\n",
      "video 1/1 (frame 357/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 56.6ms\n",
      "video 1/1 (frame 358/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 71.1ms\n",
      "video 1/1 (frame 359/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 51.6ms\n",
      "video 1/1 (frame 360/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 47.5ms\n",
      "video 1/1 (frame 361/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 11 Humans, 46.3ms\n",
      "video 1/1 (frame 362/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 63.3ms\n",
      "video 1/1 (frame 363/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 48.9ms\n",
      "video 1/1 (frame 364/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 48.6ms\n",
      "video 1/1 (frame 365/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 54.2ms\n",
      "video 1/1 (frame 366/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 47.7ms\n",
      "video 1/1 (frame 367/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 41.3ms\n",
      "video 1/1 (frame 368/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 48.6ms\n",
      "video 1/1 (frame 369/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 62.9ms\n",
      "video 1/1 (frame 370/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 49.0ms\n",
      "video 1/1 (frame 371/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 46.8ms\n",
      "video 1/1 (frame 372/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 10 Humans, 52.9ms\n",
      "video 1/1 (frame 373/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 62.7ms\n",
      "video 1/1 (frame 374/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 9 Humans, 61.4ms\n",
      "video 1/1 (frame 375/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 41.2ms\n",
      "video 1/1 (frame 376/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 55.1ms\n",
      "video 1/1 (frame 377/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 45.6ms\n",
      "video 1/1 (frame 378/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 61.8ms\n",
      "video 1/1 (frame 379/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 59.7ms\n",
      "video 1/1 (frame 380/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 47.6ms\n",
      "video 1/1 (frame 381/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 65.0ms\n",
      "video 1/1 (frame 382/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 50.5ms\n",
      "video 1/1 (frame 383/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 64.9ms\n",
      "video 1/1 (frame 384/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 44.4ms\n",
      "video 1/1 (frame 385/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 51.8ms\n",
      "video 1/1 (frame 386/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 48.8ms\n",
      "video 1/1 (frame 387/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 63.4ms\n",
      "video 1/1 (frame 388/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 8 Humans, 49.2ms\n",
      "video 1/1 (frame 389/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 61.7ms\n",
      "video 1/1 (frame 390/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 39.9ms\n",
      "video 1/1 (frame 391/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 45.4ms\n",
      "video 1/1 (frame 392/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 39.2ms\n",
      "video 1/1 (frame 393/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 51.6ms\n",
      "video 1/1 (frame 394/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 53.2ms\n",
      "video 1/1 (frame 395/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 40.5ms\n",
      "video 1/1 (frame 396/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 44.9ms\n",
      "video 1/1 (frame 397/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 44.6ms\n",
      "video 1/1 (frame 398/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 47.3ms\n",
      "video 1/1 (frame 399/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 48.2ms\n",
      "video 1/1 (frame 400/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 66.0ms\n",
      "video 1/1 (frame 401/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 52.5ms\n",
      "video 1/1 (frame 402/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 61.3ms\n",
      "video 1/1 (frame 403/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 39.7ms\n",
      "video 1/1 (frame 404/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 47.1ms\n",
      "video 1/1 (frame 405/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 65.0ms\n",
      "video 1/1 (frame 406/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 41.9ms\n",
      "video 1/1 (frame 407/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 7 Humans, 61.2ms\n",
      "video 1/1 (frame 408/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 47.4ms\n",
      "video 1/1 (frame 409/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 45.5ms\n",
      "video 1/1 (frame 410/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 56.0ms\n",
      "video 1/1 (frame 411/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 66.7ms\n",
      "video 1/1 (frame 412/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 45.3ms\n",
      "video 1/1 (frame 413/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 65.7ms\n",
      "video 1/1 (frame 414/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 60.6ms\n",
      "video 1/1 (frame 415/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 66.4ms\n",
      "video 1/1 (frame 416/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 6 Humans, 51.1ms\n",
      "video 1/1 (frame 417/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 55.8ms\n",
      "video 1/1 (frame 418/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 56.4ms\n",
      "video 1/1 (frame 419/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 4 Humans, 63.1ms\n",
      "video 1/1 (frame 420/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 64.0ms\n",
      "video 1/1 (frame 421/421) c:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\YOLOv8-HumanDetection\\input\\A001_09031158_C021.mp4: 384x640 5 Humans, 61.7ms\n",
      "Speed: 2.8ms preprocess, 50.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\paulo\\OneDrive\\CC\\Universidad de Sevilla\\PID_PocesamientoDeIamgenesDigitales\\proyecto\\runs\\detect\\predict3\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Iterate over each 'result' in the 'results' collection\n",
    "for result in results:\n",
    "    # Extract bounding box information from the current 'result'\n",
    "    boxes = result.boxes\n",
    "    # Extract mask information from the current 'result'\n",
    "    masks = result.masks\n",
    "    # Extract keypoints information from the current 'result'\n",
    "    keypoints = result.keypoints\n",
    "    # Extract probability scores from the current 'result'\n",
    "    probs = result.probs\n",
    "    \n",
    "# Now you can use 'boxes', 'masks', 'keypoints', and 'probs' as needed for further processing or analysis.\n",
    "# These variables hold the relevant information related to the object detection results for the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressione 'q' para sair\n",
      "\n",
      "0: 384x640 1 Human, 36.7ms\n",
      "Speed: 3.7ms preprocess, 36.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.6ms\n",
      "Speed: 0.8ms preprocess, 21.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.4ms\n",
      "Speed: 1.3ms preprocess, 22.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.9ms\n",
      "Speed: 1.5ms preprocess, 27.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.4ms\n",
      "Speed: 1.0ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.9ms\n",
      "Speed: 1.5ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.8ms\n",
      "Speed: 1.4ms preprocess, 23.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.8ms\n",
      "Speed: 0.8ms preprocess, 21.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.8ms\n",
      "Speed: 1.0ms preprocess, 22.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.6ms\n",
      "Speed: 1.4ms preprocess, 27.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.6ms\n",
      "Speed: 1.4ms preprocess, 28.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.6ms\n",
      "Speed: 1.5ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.0ms\n",
      "Speed: 0.9ms preprocess, 23.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.4ms\n",
      "Speed: 1.1ms preprocess, 26.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.0ms\n",
      "Speed: 1.3ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.7ms\n",
      "Speed: 1.2ms preprocess, 26.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.5ms\n",
      "Speed: 1.3ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.5ms\n",
      "Speed: 1.4ms preprocess, 29.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.2ms\n",
      "Speed: 1.4ms preprocess, 31.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.1ms\n",
      "Speed: 1.0ms preprocess, 23.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.1ms\n",
      "Speed: 0.8ms preprocess, 22.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.4ms\n",
      "Speed: 0.8ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.9ms\n",
      "Speed: 0.8ms preprocess, 23.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.5ms\n",
      "Speed: 1.3ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.5ms\n",
      "Speed: 0.9ms preprocess, 25.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.8ms\n",
      "Speed: 1.4ms preprocess, 27.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.0ms\n",
      "Speed: 1.1ms preprocess, 24.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.3ms\n",
      "Speed: 1.4ms preprocess, 28.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.2ms\n",
      "Speed: 1.3ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.2ms\n",
      "Speed: 1.1ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.3ms preprocess, 24.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.6ms\n",
      "Speed: 0.9ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.4ms\n",
      "Speed: 1.2ms preprocess, 24.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.4ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 33.4ms\n",
      "Speed: 1.5ms preprocess, 33.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.2ms\n",
      "Speed: 1.6ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.4ms\n",
      "Speed: 1.5ms preprocess, 22.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.7ms\n",
      "Speed: 0.8ms preprocess, 30.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.9ms\n",
      "Speed: 1.0ms preprocess, 27.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.5ms\n",
      "Speed: 1.3ms preprocess, 29.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.0ms\n",
      "Speed: 1.5ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.5ms\n",
      "Speed: 1.5ms preprocess, 23.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.2ms\n",
      "Speed: 0.8ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.7ms\n",
      "Speed: 1.4ms preprocess, 28.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.9ms\n",
      "Speed: 0.9ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.4ms\n",
      "Speed: 1.4ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.1ms\n",
      "Speed: 0.8ms preprocess, 22.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.4ms\n",
      "Speed: 1.4ms preprocess, 26.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.6ms\n",
      "Speed: 1.4ms preprocess, 21.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.0ms\n",
      "Speed: 1.4ms preprocess, 22.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.3ms\n",
      "Speed: 1.2ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.8ms\n",
      "Speed: 1.4ms preprocess, 25.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.7ms\n",
      "Speed: 1.5ms preprocess, 30.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.6ms\n",
      "Speed: 1.4ms preprocess, 23.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.0ms\n",
      "Speed: 0.9ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.8ms\n",
      "Speed: 1.5ms preprocess, 24.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.5ms\n",
      "Speed: 1.3ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.7ms\n",
      "Speed: 0.8ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.0ms\n",
      "Speed: 0.8ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.8ms\n",
      "Speed: 1.0ms preprocess, 21.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 32.1ms\n",
      "Speed: 1.4ms preprocess, 32.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 27.6ms\n",
      "Speed: 1.4ms preprocess, 27.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.6ms\n",
      "Speed: 1.0ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.8ms\n",
      "Speed: 1.5ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.7ms\n",
      "Speed: 1.4ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.0ms\n",
      "Speed: 1.5ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.2ms\n",
      "Speed: 0.9ms preprocess, 22.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.6ms\n",
      "Speed: 1.0ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.8ms\n",
      "Speed: 0.9ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.1ms\n",
      "Speed: 0.9ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.2ms\n",
      "Speed: 1.4ms preprocess, 30.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.9ms\n",
      "Speed: 0.9ms preprocess, 27.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.0ms\n",
      "Speed: 1.7ms preprocess, 28.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 44.8ms\n",
      "Speed: 1.5ms preprocess, 44.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.6ms\n",
      "Speed: 1.7ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.5ms\n",
      "Speed: 0.9ms preprocess, 22.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 31.6ms\n",
      "Speed: 1.5ms preprocess, 31.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.1ms\n",
      "Speed: 1.4ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.9ms\n",
      "Speed: 1.5ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.2ms\n",
      "Speed: 1.4ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.8ms\n",
      "Speed: 0.9ms preprocess, 22.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 34.0ms\n",
      "Speed: 1.4ms preprocess, 34.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.5ms\n",
      "Speed: 1.4ms preprocess, 26.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.6ms\n",
      "Speed: 1.5ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 32.0ms\n",
      "Speed: 1.4ms preprocess, 32.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.5ms\n",
      "Speed: 0.9ms preprocess, 23.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.6ms\n",
      "Speed: 1.3ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.4ms\n",
      "Speed: 0.8ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.9ms\n",
      "Speed: 1.4ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.2ms\n",
      "Speed: 2.0ms preprocess, 28.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 37.7ms\n",
      "Speed: 2.1ms preprocess, 37.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.2ms\n",
      "Speed: 1.0ms preprocess, 30.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 32.6ms\n",
      "Speed: 1.4ms preprocess, 32.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.4ms\n",
      "Speed: 1.7ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.0ms\n",
      "Speed: 1.7ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.6ms\n",
      "Speed: 1.4ms preprocess, 26.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 29.7ms\n",
      "Speed: 1.4ms preprocess, 29.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.0ms\n",
      "Speed: 1.5ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.7ms\n",
      "Speed: 0.9ms preprocess, 22.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 28.9ms\n",
      "Speed: 0.9ms preprocess, 28.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.6ms\n",
      "Speed: 1.0ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 28.0ms\n",
      "Speed: 1.4ms preprocess, 28.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.5ms\n",
      "Speed: 1.5ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 27.2ms\n",
      "Speed: 1.4ms preprocess, 27.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 27.2ms\n",
      "Speed: 1.8ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.0ms\n",
      "Speed: 1.4ms preprocess, 25.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 27.9ms\n",
      "Speed: 0.9ms preprocess, 27.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.8ms\n",
      "Speed: 0.9ms preprocess, 22.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 31.1ms\n",
      "Speed: 1.4ms preprocess, 31.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.4ms\n",
      "Speed: 1.3ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 33.4ms\n",
      "Speed: 1.4ms preprocess, 33.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 29.7ms\n",
      "Speed: 1.6ms preprocess, 29.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.3ms\n",
      "Speed: 1.2ms preprocess, 22.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.1ms\n",
      "Speed: 1.4ms preprocess, 22.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.6ms\n",
      "Speed: 0.9ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.3ms\n",
      "Speed: 0.8ms preprocess, 21.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.1ms\n",
      "Speed: 0.8ms preprocess, 22.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.0ms\n",
      "Speed: 1.0ms preprocess, 22.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 29.1ms\n",
      "Speed: 1.7ms preprocess, 29.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.2ms\n",
      "Speed: 1.4ms preprocess, 23.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 28.4ms\n",
      "Speed: 1.7ms preprocess, 28.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.9ms\n",
      "Speed: 1.3ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.7ms\n",
      "Speed: 1.4ms preprocess, 30.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.5ms\n",
      "Speed: 1.4ms preprocess, 21.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.8ms\n",
      "Speed: 1.8ms preprocess, 22.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 33.5ms\n",
      "Speed: 1.4ms preprocess, 33.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.6ms\n",
      "Speed: 1.5ms preprocess, 26.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.5ms\n",
      "Speed: 0.8ms preprocess, 22.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.6ms\n",
      "Speed: 1.6ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.2ms\n",
      "Speed: 0.8ms preprocess, 23.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.7ms\n",
      "Speed: 0.8ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.6ms\n",
      "Speed: 1.2ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 32.3ms\n",
      "Speed: 1.4ms preprocess, 32.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.5ms\n",
      "Speed: 1.4ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.9ms\n",
      "Speed: 1.4ms preprocess, 29.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.9ms\n",
      "Speed: 1.1ms preprocess, 25.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.6ms\n",
      "Speed: 0.9ms preprocess, 23.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.4ms\n",
      "Speed: 1.6ms preprocess, 26.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.7ms\n",
      "Speed: 1.5ms preprocess, 26.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.2ms\n",
      "Speed: 1.6ms preprocess, 22.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.5ms\n",
      "Speed: 0.9ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.1ms\n",
      "Speed: 1.5ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.4ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.5ms\n",
      "Speed: 1.4ms preprocess, 27.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.1ms\n",
      "Speed: 1.2ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.0ms\n",
      "Speed: 1.6ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.0ms\n",
      "Speed: 2.0ms preprocess, 30.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.1ms\n",
      "Speed: 1.2ms preprocess, 23.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.3ms\n",
      "Speed: 0.9ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.7ms\n",
      "Speed: 1.4ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.9ms\n",
      "Speed: 1.4ms preprocess, 30.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.5ms\n",
      "Speed: 1.4ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.9ms\n",
      "Speed: 1.6ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 33.8ms\n",
      "Speed: 1.3ms preprocess, 33.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 34.4ms\n",
      "Speed: 1.4ms preprocess, 34.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 35.4ms\n",
      "Speed: 1.6ms preprocess, 35.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 38.2ms\n",
      "Speed: 1.3ms preprocess, 38.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 35.7ms\n",
      "Speed: 1.3ms preprocess, 35.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 34.1ms\n",
      "Speed: 1.4ms preprocess, 34.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 35.0ms\n",
      "Speed: 1.6ms preprocess, 35.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 29.2ms\n",
      "Speed: 1.4ms preprocess, 29.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 32.9ms\n",
      "Speed: 1.4ms preprocess, 32.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.4ms\n",
      "Speed: 1.6ms preprocess, 30.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 32.8ms\n",
      "Speed: 1.4ms preprocess, 32.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.3ms\n",
      "Speed: 1.1ms preprocess, 28.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.1ms\n",
      "Speed: 1.3ms preprocess, 29.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 32.3ms\n",
      "Speed: 1.9ms preprocess, 32.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 34.4ms\n",
      "Speed: 1.4ms preprocess, 34.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 34.6ms\n",
      "Speed: 1.6ms preprocess, 34.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 35.0ms\n",
      "Speed: 1.3ms preprocess, 35.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 33.2ms\n",
      "Speed: 1.3ms preprocess, 33.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.7ms\n",
      "Speed: 1.3ms preprocess, 30.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 34.2ms\n",
      "Speed: 1.6ms preprocess, 34.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 33.9ms\n",
      "Speed: 1.2ms preprocess, 33.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.1ms\n",
      "Speed: 1.4ms preprocess, 29.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.2ms\n",
      "Speed: 1.5ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.9ms\n",
      "Speed: 1.4ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.0ms\n",
      "Speed: 1.3ms preprocess, 24.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.4ms\n",
      "Speed: 1.3ms preprocess, 23.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.1ms\n",
      "Speed: 0.8ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.1ms\n",
      "Speed: 1.5ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.9ms\n",
      "Speed: 1.3ms preprocess, 24.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.3ms\n",
      "Speed: 0.8ms preprocess, 23.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.0ms\n",
      "Speed: 1.4ms preprocess, 27.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.4ms\n",
      "Speed: 0.9ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.8ms\n",
      "Speed: 1.4ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 34.5ms\n",
      "Speed: 1.9ms preprocess, 34.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.3ms\n",
      "Speed: 1.6ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.9ms\n",
      "Speed: 1.4ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 33.4ms\n",
      "Speed: 1.4ms preprocess, 33.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 20.8ms\n",
      "Speed: 0.9ms preprocess, 20.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.1ms\n",
      "Speed: 0.9ms preprocess, 25.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.8ms\n",
      "Speed: 1.3ms preprocess, 32.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.5ms\n",
      "Speed: 1.5ms preprocess, 26.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.6ms\n",
      "Speed: 1.3ms preprocess, 29.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 26.1ms\n",
      "Speed: 1.5ms preprocess, 26.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.2ms\n",
      "Speed: 1.4ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.4ms\n",
      "Speed: 0.8ms preprocess, 25.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.1ms\n",
      "Speed: 1.4ms preprocess, 21.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 28.8ms\n",
      "Speed: 1.4ms preprocess, 28.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 Humans, 27.2ms\n",
      "Speed: 1.4ms preprocess, 27.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.7ms\n",
      "Speed: 1.4ms preprocess, 25.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.4ms\n",
      "Speed: 1.3ms preprocess, 27.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.2ms\n",
      "Speed: 1.6ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.5ms\n",
      "Speed: 1.5ms preprocess, 23.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 22.7ms\n",
      "Speed: 0.8ms preprocess, 22.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.0ms\n",
      "Speed: 1.7ms preprocess, 27.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 33.0ms\n",
      "Speed: 1.3ms preprocess, 33.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.1ms\n",
      "Speed: 1.3ms preprocess, 25.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.2ms\n",
      "Speed: 1.4ms preprocess, 25.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.0ms\n",
      "Speed: 1.4ms preprocess, 24.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.7ms\n",
      "Speed: 1.4ms preprocess, 24.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.5ms\n",
      "Speed: 1.3ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.0ms\n",
      "Speed: 0.9ms preprocess, 23.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.6ms\n",
      "Speed: 1.5ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 20.9ms\n",
      "Speed: 0.9ms preprocess, 20.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.7ms\n",
      "Speed: 1.4ms preprocess, 27.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.3ms\n",
      "Speed: 1.5ms preprocess, 27.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.4ms\n",
      "Speed: 0.9ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 30.6ms\n",
      "Speed: 1.4ms preprocess, 30.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.0ms\n",
      "Speed: 1.4ms preprocess, 27.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.5ms\n",
      "Speed: 1.0ms preprocess, 21.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.4ms\n",
      "Speed: 1.3ms preprocess, 25.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.9ms\n",
      "Speed: 1.4ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.0ms\n",
      "Speed: 0.8ms preprocess, 23.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.8ms\n",
      "Speed: 1.5ms preprocess, 24.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.1ms\n",
      "Speed: 1.8ms preprocess, 22.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 1.6ms preprocess, 21.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.9ms\n",
      "Speed: 1.3ms preprocess, 25.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.3ms\n",
      "Speed: 0.8ms preprocess, 22.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 22.1ms\n",
      "Speed: 0.9ms preprocess, 22.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 28.2ms\n",
      "Speed: 1.4ms preprocess, 28.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 26.4ms\n",
      "Speed: 0.8ms preprocess, 26.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.8ms\n",
      "Speed: 1.3ms preprocess, 23.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.9ms\n",
      "Speed: 0.8ms preprocess, 20.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.7ms\n",
      "Speed: 1.4ms preprocess, 22.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.7ms\n",
      "Speed: 1.2ms preprocess, 23.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.8ms\n",
      "Speed: 1.0ms preprocess, 21.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.5ms\n",
      "Speed: 0.8ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.8ms\n",
      "Speed: 1.0ms preprocess, 26.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.7ms\n",
      "Speed: 0.8ms preprocess, 24.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.7ms\n",
      "Speed: 0.9ms preprocess, 23.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 31.3ms\n",
      "Speed: 1.4ms preprocess, 31.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.4ms\n",
      "Speed: 0.8ms preprocess, 23.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.5ms\n",
      "Speed: 0.8ms preprocess, 20.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.7ms\n",
      "Speed: 0.9ms preprocess, 21.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.8ms\n",
      "Speed: 0.9ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.7ms\n",
      "Speed: 1.7ms preprocess, 21.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.4ms\n",
      "Speed: 1.0ms preprocess, 21.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.6ms\n",
      "Speed: 1.3ms preprocess, 21.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 0.8ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.4ms\n",
      "Speed: 1.4ms preprocess, 22.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.1ms\n",
      "Speed: 0.9ms preprocess, 20.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.7ms\n",
      "Speed: 1.1ms preprocess, 25.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.7ms\n",
      "Speed: 1.0ms preprocess, 22.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 33.0ms\n",
      "Speed: 1.3ms preprocess, 33.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.3ms\n",
      "Speed: 1.3ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.6ms\n",
      "Speed: 0.9ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.6ms\n",
      "Speed: 1.4ms preprocess, 26.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.1ms\n",
      "Speed: 1.5ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.6ms\n",
      "Speed: 1.4ms preprocess, 27.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 28.4ms\n",
      "Speed: 1.0ms preprocess, 28.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.1ms\n",
      "Speed: 1.5ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 43.3ms\n",
      "Speed: 1.4ms preprocess, 43.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.2ms\n",
      "Speed: 1.5ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.0ms\n",
      "Speed: 1.3ms preprocess, 27.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.7ms\n",
      "Speed: 1.4ms preprocess, 27.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.5ms\n",
      "Speed: 1.5ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.9ms\n",
      "Speed: 1.4ms preprocess, 27.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 1.4ms preprocess, 22.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.7ms\n",
      "Speed: 1.4ms preprocess, 26.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.5ms\n",
      "Speed: 1.5ms preprocess, 29.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.3ms\n",
      "Speed: 1.8ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.6ms\n",
      "Speed: 0.9ms preprocess, 23.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.5ms\n",
      "Speed: 1.0ms preprocess, 21.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.2ms\n",
      "Speed: 1.1ms preprocess, 24.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.0ms\n",
      "Speed: 1.4ms preprocess, 23.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 30.5ms\n",
      "Speed: 1.5ms preprocess, 30.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.5ms\n",
      "Speed: 1.7ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.7ms\n",
      "Speed: 1.3ms preprocess, 29.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.2ms\n",
      "Speed: 1.6ms preprocess, 31.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.5ms\n",
      "Speed: 1.0ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.1ms\n",
      "Speed: 1.4ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.2ms\n",
      "Speed: 1.0ms preprocess, 22.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.6ms\n",
      "Speed: 1.8ms preprocess, 26.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.7ms\n",
      "Speed: 1.5ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.7ms\n",
      "Speed: 0.8ms preprocess, 21.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.1ms\n",
      "Speed: 1.4ms preprocess, 22.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.5ms\n",
      "Speed: 1.4ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.6ms\n",
      "Speed: 0.9ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.1ms\n",
      "Speed: 1.4ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.6ms\n",
      "Speed: 0.8ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.2ms\n",
      "Speed: 0.8ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.9ms\n",
      "Speed: 1.3ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.2ms\n",
      "Speed: 1.4ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 20.9ms\n",
      "Speed: 0.9ms preprocess, 20.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.0ms\n",
      "Speed: 1.0ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.7ms\n",
      "Speed: 1.4ms preprocess, 24.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.6ms\n",
      "Speed: 0.9ms preprocess, 22.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.8ms\n",
      "Speed: 0.9ms preprocess, 27.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.2ms\n",
      "Speed: 1.2ms preprocess, 28.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.1ms\n",
      "Speed: 1.0ms preprocess, 22.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.3ms\n",
      "Speed: 1.0ms preprocess, 22.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.6ms\n",
      "Speed: 1.7ms preprocess, 23.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 20.8ms\n",
      "Speed: 1.1ms preprocess, 20.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 32.5ms\n",
      "Speed: 1.4ms preprocess, 32.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.1ms\n",
      "Speed: 1.3ms preprocess, 29.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.5ms\n",
      "Speed: 0.9ms preprocess, 22.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.8ms\n",
      "Speed: 1.4ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.4ms\n",
      "Speed: 1.3ms preprocess, 30.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.9ms\n",
      "Speed: 1.4ms preprocess, 29.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.4ms\n",
      "Speed: 1.3ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.0ms\n",
      "Speed: 1.5ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.4ms\n",
      "Speed: 0.9ms preprocess, 22.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.7ms\n",
      "Speed: 1.7ms preprocess, 22.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.5ms\n",
      "Speed: 0.9ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.5ms\n",
      "Speed: 1.3ms preprocess, 31.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.4ms\n",
      "Speed: 0.8ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.4ms\n",
      "Speed: 1.4ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.0ms\n",
      "Speed: 1.2ms preprocess, 21.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.0ms\n",
      "Speed: 1.4ms preprocess, 24.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.7ms\n",
      "Speed: 0.9ms preprocess, 21.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.3ms\n",
      "Speed: 1.4ms preprocess, 25.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.3ms\n",
      "Speed: 1.0ms preprocess, 22.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.6ms\n",
      "Speed: 1.6ms preprocess, 22.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.2ms\n",
      "Speed: 1.5ms preprocess, 29.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.8ms\n",
      "Speed: 1.0ms preprocess, 20.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.0ms\n",
      "Speed: 1.1ms preprocess, 23.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 36.3ms\n",
      "Speed: 1.4ms preprocess, 36.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.6ms\n",
      "Speed: 1.6ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.8ms\n",
      "Speed: 1.0ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 21.9ms\n",
      "Speed: 1.1ms preprocess, 21.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.8ms\n",
      "Speed: 1.5ms preprocess, 22.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.9ms\n",
      "Speed: 0.9ms preprocess, 26.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 21.0ms\n",
      "Speed: 1.0ms preprocess, 21.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 21.1ms\n",
      "Speed: 1.7ms preprocess, 21.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.6ms\n",
      "Speed: 1.4ms preprocess, 23.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.2ms\n",
      "Speed: 1.4ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.9ms\n",
      "Speed: 0.8ms preprocess, 20.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 30.2ms\n",
      "Speed: 1.4ms preprocess, 30.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.4ms\n",
      "Speed: 1.3ms preprocess, 26.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 1.4ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.3ms\n",
      "Speed: 1.5ms preprocess, 22.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.3ms\n",
      "Speed: 1.4ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.5ms\n",
      "Speed: 1.2ms preprocess, 23.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 28.3ms\n",
      "Speed: 1.4ms preprocess, 28.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 21.3ms\n",
      "Speed: 0.9ms preprocess, 21.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.5ms\n",
      "Speed: 0.9ms preprocess, 23.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 20.8ms\n",
      "Speed: 1.5ms preprocess, 20.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 30.1ms\n",
      "Speed: 1.3ms preprocess, 30.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 32.7ms\n",
      "Speed: 1.4ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.3ms\n",
      "Speed: 1.6ms preprocess, 27.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.7ms\n",
      "Speed: 1.5ms preprocess, 23.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.9ms\n",
      "Speed: 1.5ms preprocess, 25.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.0ms\n",
      "Speed: 1.5ms preprocess, 23.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 26.2ms\n",
      "Speed: 1.4ms preprocess, 26.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 29.7ms\n",
      "Speed: 1.4ms preprocess, 29.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 31.8ms\n",
      "Speed: 1.4ms preprocess, 31.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 22.0ms\n",
      "Speed: 0.9ms preprocess, 22.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 31.8ms\n",
      "Speed: 1.3ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.7ms\n",
      "Speed: 1.4ms preprocess, 29.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 1.4ms preprocess, 21.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.4ms\n",
      "Speed: 1.3ms preprocess, 24.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.7ms\n",
      "Speed: 1.4ms preprocess, 32.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.3ms\n",
      "Speed: 1.5ms preprocess, 23.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.7ms\n",
      "Speed: 1.3ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 26.7ms\n",
      "Speed: 1.4ms preprocess, 26.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.2ms\n",
      "Speed: 1.2ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.6ms\n",
      "Speed: 1.8ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.3ms\n",
      "Speed: 1.5ms preprocess, 28.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.4ms\n",
      "Speed: 1.4ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.5ms\n",
      "Speed: 1.5ms preprocess, 22.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.1ms\n",
      "Speed: 1.0ms preprocess, 22.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.6ms\n",
      "Speed: 1.1ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 27.2ms\n",
      "Speed: 1.4ms preprocess, 27.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.1ms\n",
      "Speed: 1.3ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.1ms\n",
      "Speed: 1.6ms preprocess, 31.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.3ms\n",
      "Speed: 1.4ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.8ms\n",
      "Speed: 0.9ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.8ms\n",
      "Speed: 0.9ms preprocess, 22.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.6ms\n",
      "Speed: 1.4ms preprocess, 27.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.4ms\n",
      "Speed: 1.1ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.2ms\n",
      "Speed: 0.9ms preprocess, 22.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 30.4ms\n",
      "Speed: 1.4ms preprocess, 30.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.8ms\n",
      "Speed: 1.4ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 37.9ms\n",
      "Speed: 1.4ms preprocess, 37.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.3ms\n",
      "Speed: 1.0ms preprocess, 25.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 33.1ms\n",
      "Speed: 1.5ms preprocess, 33.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 21.7ms\n",
      "Speed: 1.2ms preprocess, 21.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.8ms\n",
      "Speed: 1.6ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.3ms\n",
      "Speed: 0.9ms preprocess, 24.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.2ms\n",
      "Speed: 1.5ms preprocess, 23.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.8ms\n",
      "Speed: 1.4ms preprocess, 25.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.3ms\n",
      "Speed: 1.5ms preprocess, 22.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.5ms\n",
      "Speed: 0.9ms preprocess, 21.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.0ms\n",
      "Speed: 1.5ms preprocess, 26.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.0ms\n",
      "Speed: 1.5ms preprocess, 23.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.5ms\n",
      "Speed: 1.4ms preprocess, 26.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.1ms\n",
      "Speed: 0.8ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 28.0ms\n",
      "Speed: 1.4ms preprocess, 28.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.3ms\n",
      "Speed: 1.3ms preprocess, 23.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.5ms\n",
      "Speed: 1.4ms preprocess, 25.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 28.8ms\n",
      "Speed: 1.4ms preprocess, 28.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.4ms\n",
      "Speed: 1.1ms preprocess, 21.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.1ms\n",
      "Speed: 1.3ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.4ms\n",
      "Speed: 1.4ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.7ms\n",
      "Speed: 1.5ms preprocess, 29.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.6ms\n",
      "Speed: 0.9ms preprocess, 20.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.8ms\n",
      "Speed: 0.9ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.5ms\n",
      "Speed: 1.5ms preprocess, 22.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.6ms\n",
      "Speed: 1.4ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.8ms\n",
      "Speed: 1.4ms preprocess, 27.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 22.6ms\n",
      "Speed: 0.9ms preprocess, 22.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 22.2ms\n",
      "Speed: 0.9ms preprocess, 22.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.5ms\n",
      "Speed: 1.1ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.2ms\n",
      "Speed: 1.4ms preprocess, 21.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.1ms\n",
      "Speed: 1.4ms preprocess, 22.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.7ms\n",
      "Speed: 1.5ms preprocess, 26.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.0ms\n",
      "Speed: 1.3ms preprocess, 25.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.6ms\n",
      "Speed: 1.9ms preprocess, 22.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.8ms\n",
      "Speed: 1.4ms preprocess, 24.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.0ms\n",
      "Speed: 1.5ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.6ms\n",
      "Speed: 1.3ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.1ms\n",
      "Speed: 1.4ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.9ms\n",
      "Speed: 1.1ms preprocess, 20.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.4ms\n",
      "Speed: 1.4ms preprocess, 23.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 30.2ms\n",
      "Speed: 1.4ms preprocess, 30.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.8ms\n",
      "Speed: 1.4ms preprocess, 26.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.0ms\n",
      "Speed: 1.6ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 33.3ms\n",
      "Speed: 1.3ms preprocess, 33.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.1ms\n",
      "Speed: 1.5ms preprocess, 32.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 19.8ms\n",
      "Speed: 0.9ms preprocess, 19.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.8ms\n",
      "Speed: 0.9ms preprocess, 21.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 22.3ms\n",
      "Speed: 1.6ms preprocess, 22.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.8ms\n",
      "Speed: 1.6ms preprocess, 21.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 28.9ms\n",
      "Speed: 1.5ms preprocess, 28.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.8ms\n",
      "Speed: 1.4ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 31.1ms\n",
      "Speed: 1.5ms preprocess, 31.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 30.7ms\n",
      "Speed: 0.9ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.6ms\n",
      "Speed: 1.8ms preprocess, 23.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.1ms\n",
      "Speed: 1.4ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 28.5ms\n",
      "Speed: 1.4ms preprocess, 28.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 21.6ms\n",
      "Speed: 0.8ms preprocess, 21.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.2ms\n",
      "Speed: 1.4ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 26.3ms\n",
      "Speed: 1.6ms preprocess, 26.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.3ms\n",
      "Speed: 1.6ms preprocess, 24.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.5ms\n",
      "Speed: 1.8ms preprocess, 22.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.7ms\n",
      "Speed: 1.4ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.1ms\n",
      "Speed: 1.4ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 1.0ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.6ms\n",
      "Speed: 1.4ms preprocess, 32.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.1ms\n",
      "Speed: 1.5ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.4ms\n",
      "Speed: 1.4ms preprocess, 27.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.0ms\n",
      "Speed: 0.8ms preprocess, 24.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 32.4ms\n",
      "Speed: 1.5ms preprocess, 32.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.5ms\n",
      "Speed: 1.0ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.8ms\n",
      "Speed: 1.3ms preprocess, 27.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.3ms\n",
      "Speed: 0.9ms preprocess, 23.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.8ms\n",
      "Speed: 1.5ms preprocess, 22.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.1ms\n",
      "Speed: 1.2ms preprocess, 22.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 1.0ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.4ms\n",
      "Speed: 1.2ms preprocess, 26.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.1ms\n",
      "Speed: 1.4ms preprocess, 26.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 0.8ms preprocess, 21.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 0.9ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 28.5ms\n",
      "Speed: 1.5ms preprocess, 28.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 31.2ms\n",
      "Speed: 1.3ms preprocess, 31.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.5ms\n",
      "Speed: 1.0ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.4ms\n",
      "Speed: 0.9ms preprocess, 26.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.1ms\n",
      "Speed: 1.3ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.7ms\n",
      "Speed: 1.7ms preprocess, 27.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.7ms\n",
      "Speed: 0.9ms preprocess, 22.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 0.9ms preprocess, 21.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.0ms\n",
      "Speed: 1.5ms preprocess, 29.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 28.8ms\n",
      "Speed: 1.3ms preprocess, 28.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.5ms\n",
      "Speed: 1.5ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 29.8ms\n",
      "Speed: 1.5ms preprocess, 29.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.6ms\n",
      "Speed: 1.1ms preprocess, 23.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.8ms\n",
      "Speed: 1.4ms preprocess, 26.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.8ms\n",
      "Speed: 0.8ms preprocess, 20.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.9ms\n",
      "Speed: 1.2ms preprocess, 25.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.6ms\n",
      "Speed: 1.5ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.1ms\n",
      "Speed: 1.4ms preprocess, 23.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.8ms\n",
      "Speed: 1.0ms preprocess, 21.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.0ms\n",
      "Speed: 0.8ms preprocess, 20.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.7ms\n",
      "Speed: 1.4ms preprocess, 20.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.6ms\n",
      "Speed: 1.3ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.0ms\n",
      "Speed: 1.8ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 40.8ms\n",
      "Speed: 1.6ms preprocess, 40.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.7ms\n",
      "Speed: 1.4ms preprocess, 27.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 25.0ms\n",
      "Speed: 0.9ms preprocess, 25.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 23.6ms\n",
      "Speed: 1.3ms preprocess, 23.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.4ms\n",
      "Speed: 1.3ms preprocess, 25.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.5ms\n",
      "Speed: 0.9ms preprocess, 20.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 33.3ms\n",
      "Speed: 1.4ms preprocess, 33.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.8ms\n",
      "Speed: 1.5ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.4ms preprocess, 24.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.6ms\n",
      "Speed: 1.4ms preprocess, 24.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.4ms\n",
      "Speed: 1.4ms preprocess, 26.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.1ms\n",
      "Speed: 1.4ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.3ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.1ms\n",
      "Speed: 1.6ms preprocess, 28.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.2ms\n",
      "Speed: 1.4ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.9ms\n",
      "Speed: 1.5ms preprocess, 24.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 0.9ms preprocess, 21.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.8ms\n",
      "Speed: 1.1ms preprocess, 21.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.8ms\n",
      "Speed: 1.7ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.1ms\n",
      "Speed: 1.4ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.0ms\n",
      "Speed: 0.8ms preprocess, 22.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.2ms\n",
      "Speed: 1.7ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.2ms\n",
      "Speed: 1.3ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.9ms\n",
      "Speed: 1.0ms preprocess, 22.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.2ms\n",
      "Speed: 1.3ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.6ms\n",
      "Speed: 1.3ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.1ms\n",
      "Speed: 1.4ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.8ms\n",
      "Speed: 1.4ms preprocess, 28.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.5ms\n",
      "Speed: 1.4ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.8ms\n",
      "Speed: 1.7ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.6ms\n",
      "Speed: 1.5ms preprocess, 22.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.0ms\n",
      "Speed: 1.4ms preprocess, 25.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.6ms\n",
      "Speed: 0.8ms preprocess, 20.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.2ms\n",
      "Speed: 1.5ms preprocess, 21.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.4ms\n",
      "Speed: 1.4ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 28.8ms\n",
      "Speed: 1.6ms preprocess, 28.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.3ms\n",
      "Speed: 0.9ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.6ms\n",
      "Speed: 1.3ms preprocess, 22.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.8ms\n",
      "Speed: 1.5ms preprocess, 22.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 28.7ms\n",
      "Speed: 1.4ms preprocess, 28.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.3ms\n",
      "Speed: 1.6ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.8ms\n",
      "Speed: 1.4ms preprocess, 21.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 0.9ms preprocess, 21.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.5ms\n",
      "Speed: 1.3ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.8ms\n",
      "Speed: 1.4ms preprocess, 27.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.9ms\n",
      "Speed: 1.4ms preprocess, 24.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 37.8ms\n",
      "Speed: 1.0ms preprocess, 37.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.5ms\n",
      "Speed: 1.1ms preprocess, 27.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.0ms\n",
      "Speed: 0.8ms preprocess, 23.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.5ms\n",
      "Speed: 1.9ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.8ms\n",
      "Speed: 1.5ms preprocess, 22.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.0ms\n",
      "Speed: 0.9ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.9ms\n",
      "Speed: 1.1ms preprocess, 21.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.9ms\n",
      "Speed: 1.1ms preprocess, 22.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 30.8ms\n",
      "Speed: 1.5ms preprocess, 30.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.0ms\n",
      "Speed: 0.8ms preprocess, 23.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.9ms\n",
      "Speed: 1.3ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.5ms\n",
      "Speed: 1.4ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 24.1ms\n",
      "Speed: 1.4ms preprocess, 24.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.5ms preprocess, 24.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.3ms\n",
      "Speed: 1.4ms preprocess, 30.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.7ms\n",
      "Speed: 1.5ms preprocess, 25.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.6ms\n",
      "Speed: 0.8ms preprocess, 24.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.3ms\n",
      "Speed: 1.4ms preprocess, 25.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.8ms\n",
      "Speed: 1.5ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.5ms\n",
      "Speed: 1.5ms preprocess, 27.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.7ms\n",
      "Speed: 1.4ms preprocess, 22.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.1ms\n",
      "Speed: 1.3ms preprocess, 27.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.0ms\n",
      "Speed: 1.6ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.0ms\n",
      "Speed: 1.2ms preprocess, 22.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.5ms\n",
      "Speed: 0.8ms preprocess, 22.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 32.3ms\n",
      "Speed: 1.4ms preprocess, 32.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.7ms\n",
      "Speed: 0.9ms preprocess, 21.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.6ms\n",
      "Speed: 1.7ms preprocess, 21.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.7ms\n",
      "Speed: 1.5ms preprocess, 21.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.0ms\n",
      "Speed: 1.4ms preprocess, 23.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.2ms\n",
      "Speed: 0.8ms preprocess, 23.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.9ms\n",
      "Speed: 1.4ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.7ms\n",
      "Speed: 1.7ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.2ms\n",
      "Speed: 0.8ms preprocess, 23.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.2ms\n",
      "Speed: 0.9ms preprocess, 22.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 21.2ms\n",
      "Speed: 0.8ms preprocess, 21.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.0ms\n",
      "Speed: 1.4ms preprocess, 23.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 41.2ms\n",
      "Speed: 1.0ms preprocess, 41.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 62.3ms\n",
      "Speed: 1.6ms preprocess, 62.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 42.0ms\n",
      "Speed: 2.0ms preprocess, 42.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 34.9ms\n",
      "Speed: 1.8ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 35.0ms\n",
      "Speed: 1.4ms preprocess, 35.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 31.6ms\n",
      "Speed: 1.3ms preprocess, 31.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.3ms\n",
      "Speed: 1.4ms preprocess, 31.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 34.8ms\n",
      "Speed: 1.4ms preprocess, 34.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.9ms\n",
      "Speed: 1.5ms preprocess, 30.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.5ms\n",
      "Speed: 1.0ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.8ms\n",
      "Speed: 0.8ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.0ms\n",
      "Speed: 1.4ms preprocess, 26.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.4ms\n",
      "Speed: 1.5ms preprocess, 23.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.6ms\n",
      "Speed: 1.5ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.6ms\n",
      "Speed: 0.9ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.4ms\n",
      "Speed: 1.2ms preprocess, 28.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.2ms\n",
      "Speed: 1.2ms preprocess, 23.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 30.0ms\n",
      "Speed: 1.5ms preprocess, 30.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.9ms\n",
      "Speed: 1.4ms preprocess, 22.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.0ms\n",
      "Speed: 1.4ms preprocess, 27.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 47.9ms\n",
      "Speed: 1.4ms preprocess, 47.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 39.9ms\n",
      "Speed: 2.3ms preprocess, 39.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 36.3ms\n",
      "Speed: 1.6ms preprocess, 36.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 36.1ms\n",
      "Speed: 1.5ms preprocess, 36.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 34.3ms\n",
      "Speed: 1.4ms preprocess, 34.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 31.9ms\n",
      "Speed: 1.7ms preprocess, 31.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 34.4ms\n",
      "Speed: 1.5ms preprocess, 34.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 20.8ms\n",
      "Speed: 1.5ms preprocess, 20.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.4ms\n",
      "Speed: 1.4ms preprocess, 22.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 21.9ms\n",
      "Speed: 1.4ms preprocess, 21.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.4ms\n",
      "Speed: 1.0ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.0ms\n",
      "Speed: 1.1ms preprocess, 24.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 34.5ms\n",
      "Speed: 1.4ms preprocess, 34.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 31.0ms\n",
      "Speed: 1.8ms preprocess, 31.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 34.0ms\n",
      "Speed: 1.3ms preprocess, 34.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.3ms\n",
      "Speed: 0.9ms preprocess, 22.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.8ms\n",
      "Speed: 0.9ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.8ms\n",
      "Speed: 0.9ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.1ms\n",
      "Speed: 1.5ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.5ms\n",
      "Speed: 1.4ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.3ms\n",
      "Speed: 1.5ms preprocess, 23.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.8ms\n",
      "Speed: 1.5ms preprocess, 25.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.4ms\n",
      "Speed: 1.4ms preprocess, 25.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 27.6ms\n",
      "Speed: 1.4ms preprocess, 27.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.3ms\n",
      "Speed: 1.4ms preprocess, 28.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.4ms\n",
      "Speed: 1.4ms preprocess, 25.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.2ms\n",
      "Speed: 0.8ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.1ms\n",
      "Speed: 1.5ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 31.4ms\n",
      "Speed: 1.6ms preprocess, 31.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.6ms\n",
      "Speed: 0.8ms preprocess, 23.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.3ms\n",
      "Speed: 1.4ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.1ms\n",
      "Speed: 1.6ms preprocess, 23.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.1ms\n",
      "Speed: 1.0ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 27.4ms\n",
      "Speed: 1.5ms preprocess, 27.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 23.9ms\n",
      "Speed: 0.9ms preprocess, 23.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 26.0ms\n",
      "Speed: 1.6ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 24.1ms\n",
      "Speed: 1.1ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 30.9ms\n",
      "Speed: 0.9ms preprocess, 30.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.8ms\n",
      "Speed: 1.3ms preprocess, 22.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.9ms\n",
      "Speed: 1.4ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 25.8ms\n",
      "Speed: 0.9ms preprocess, 25.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.0ms\n",
      "Speed: 0.8ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 Humans, 27.0ms\n",
      "Speed: 1.6ms preprocess, 27.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 Humans, 22.7ms\n",
      "Speed: 1.3ms preprocess, 22.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.0ms\n",
      "Speed: 1.0ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 33.2ms\n",
      "Speed: 1.4ms preprocess, 33.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.0ms\n",
      "Speed: 1.5ms preprocess, 28.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 25.1ms\n",
      "Speed: 1.4ms preprocess, 25.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.9ms\n",
      "Speed: 1.6ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.9ms\n",
      "Speed: 0.8ms preprocess, 21.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.4ms\n",
      "Speed: 0.9ms preprocess, 23.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.6ms\n",
      "Speed: 1.1ms preprocess, 21.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 21.0ms\n",
      "Speed: 1.7ms preprocess, 21.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 38.1ms\n",
      "Speed: 1.4ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 28.3ms\n",
      "Speed: 1.0ms preprocess, 28.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 23.8ms\n",
      "Speed: 0.9ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 24.4ms\n",
      "Speed: 1.3ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 22.4ms\n",
      "Speed: 0.8ms preprocess, 22.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Human, 23.4ms\n",
      "Speed: 1.0ms preprocess, 23.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.8ms\n",
      "Speed: 0.9ms preprocess, 22.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.0ms\n",
      "Speed: 1.5ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 22.0ms\n",
      "Speed: 1.3ms preprocess, 22.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 34.4ms\n",
      "Speed: 1.3ms preprocess, 34.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 20.6ms\n",
      "Speed: 0.8ms preprocess, 20.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.5ms\n",
      "Speed: 1.6ms preprocess, 29.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 25.8ms\n",
      "Speed: 1.4ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 26.5ms\n",
      "Speed: 1.4ms preprocess, 26.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.4ms\n",
      "Speed: 0.8ms preprocess, 31.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 33.8ms\n",
      "Speed: 1.7ms preprocess, 33.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 29.5ms\n",
      "Speed: 1.4ms preprocess, 29.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 31.9ms\n",
      "Speed: 0.9ms preprocess, 31.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Humans, 32.9ms\n",
      "Speed: 1.2ms preprocess, 32.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m results = model(frame, stream=\u001b[38;5;28;01mTrue\u001b[39;00m, conf=\u001b[32m0.5\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Desenhar resultados no frame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mannotated_frame\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Contar pessoas detectadas\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:328\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    330\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:658\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:154\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:176\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    177\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:307\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    305\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    306\u001b[39m y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:89\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     82\u001b[39m \n\u001b[32m     83\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:473\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    470\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\paulo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2370\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace=inplace)\n\u001b[32m   2369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m2370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Nova clula 7 - Deteco com webcam\n",
    "import cv2\n",
    "\n",
    "# Inicializar webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Configurar resoluo\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "print(\"Pressione 'q' para sair\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Erro ao capturar frame\")\n",
    "        break\n",
    "    \n",
    "    # Fazer predio no frame\n",
    "    results = model(frame, stream=True, conf=0.5)\n",
    "    \n",
    "    # Desenhar resultados no frame\n",
    "    for result in results:\n",
    "        annotated_frame = result.plot()\n",
    "        \n",
    "        # Contar pessoas detectadas\n",
    "        num_people = len(result.boxes)\n",
    "        \n",
    "        # Adicionar contador\n",
    "        cv2.putText(annotated_frame, f'Pessoas: {num_people}', \n",
    "                   (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   1, (0, 255, 0), 3)\n",
    "        \n",
    "        # Mostrar frame\n",
    "        cv2.imshow('YOLOv8 - Deteco de Humanos', annotated_frame)\n",
    "    \n",
    "    # Sair com 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Deteco encerrada\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4431250,
     "sourceId": 7610205,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4431275,
     "sourceId": 7610239,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
